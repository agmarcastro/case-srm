# ============================================
# ENVIRONMENT CONFIGURATION TEMPLATE
# ============================================

#

# ============================================
# Project Configuration
# ============================================
PROJECT_NAME=srm
APP_VERSION=1.0.0

# ============================================
# PostgreSQL Configuration
# ============================================
POSTGRES_USER=airflow
POSTGRES_PASSWORD=DevPassword123
POSTGRES_DB=airflow
POSTGRES_PORT=5432

# ============================================
# MinIO Configuration
# ============================================
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=DevPassword123
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001

# ============================================
# Airflow Configuration
# ============================================
# Generate Fernet key: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=YourFernetKeyHere123456789012345678901234=

# Generate webserver secret: openssl rand -hex 32
AIRFLOW_WEBSERVER_SECRET_KEY=devsecretkey1234567890abcdef1234567890abcdef

AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=DevPassword123
AIRFLOW_WEBSERVER_PORT=8081

# ============================================
# Spark Configuration
# ============================================
SPARK_MASTER_UI_PORT=8080
SPARK_MASTER_PORT=7077
SPARK_HISTORY_PORT=18080
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2

# Spark Security (set to 'yes' for production)
SPARK_RPC_AUTH=no
SPARK_RPC_ENCRYPT=no
SPARK_STORAGE_ENCRYPT=no
SPARK_SSL=no

# ============================================
# Storage Configuration (MinIO/S3)
# ============================================
# For local development: use MinIO as S3-compatible storage
# For production: replace with actual S3/GCS endpoint
S3_ENDPOINT=http://minio:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=DevPassword123
S3_BUCKET=cnpj-data
S3_PATH_STYLE_ACCESS=true

# Data Layer Paths (S3A via MinIO)
# MinIO bucket created automatically by minio-init container
BRONZE_LAYER_PATH=s3a://cnpj-data/bronze
SILVER_LAYER_PATH=s3a://cnpj-data/silver
GOLD_LAYER_PATH=s3a://cnpj-data/gold

# ============================================
# Spark Cluster Connection
# ============================================
# Use distributed cluster mode (Spark workers handle the heavy processing)
SPARK_MASTER_URL=spark://spark-master:7077
# For local-only mode (no cluster): SPARK_MASTER_URL=local[*]
# WARNING: local[*] runs Spark JVM inside Airflow scheduler (1GB limit) â€” causes OOM on large datasets

# ============================================
# PRODUCTION DEPLOYMENT NOTES
# ============================================
# 1. Generate all passwords using a secure random generator
# 2. For AIRFLOW_FERNET_KEY, run:
#    python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# 3. For AIRFLOW_WEBSERVER_SECRET_KEY, run:
#    openssl rand -hex 32
# 4. Store secrets in a secure vault (e.g., HashiCorp Vault, AWS Secrets Manager)
# 5. Enable Spark security features in production environments
# 6. Review and adjust resource limits in docker-compose.yml for your infrastructure
