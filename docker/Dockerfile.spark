
FROM apache/spark:3.5.0

# Metadata for security scanning and documentation
LABEL org.opencontainers.image.title="Pipeline SRM - Spark" \
      org.opencontainers.image.description="PySpark container for SRM data pipeline" \
      org.opencontainers.image.version="1.0.0" \
      org.opencontainers.image.vendor="SRM Project" \
      maintainer="SRM Team"

# Switch to root for installation only
USER root


RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        build-essential wget libssl-dev zlib1g-dev \
        libbz2-dev libreadline-dev libsqlite3-dev \
        libncursesw5-dev libxml2-dev libxmlsec1-dev \
        libffi-dev liblzma-dev && \
    wget -q https://www.python.org/ftp/python/3.11.11/Python-3.11.11.tgz && \
    tar xzf Python-3.11.11.tgz && \
    cd Python-3.11.11 && \
    ./configure --enable-optimizations --prefix=/usr/local 2>&1 | tail -1 && \
    make -j$(nproc) 2>&1 | tail -1 && \
    make altinstall && \
    cd / && rm -rf Python-3.11.11 Python-3.11.11.tgz && \
    rm -rf /var/lib/apt/lists/* && apt-get clean

# Install pip for Python 3.11
RUN python3.11 -m ensurepip --upgrade && \
    python3.11 -m pip install --no-cache-dir --upgrade pip


COPY docker/requirements-spark.txt /tmp/requirements.txt

RUN --mount=type=cache,target=/root/.cache/pip \
    python3.11 -m pip install --no-cache-dir -r /tmp/requirements.txt && \
    rm -f /tmp/requirements.txt

RUN wget -q -P /opt/spark/jars/ \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    wget -q -P /opt/spark/jars/ \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar


COPY --chown=185:root src/pipeline_srm /opt/pipeline_srm


ENV PYTHONPATH="/opt:${PYTHONPATH}" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Create data directories with proper permissions
RUN mkdir -p /opt/data/bronze /opt/data/silver /opt/data/gold /tmp/cnpj /tmp/spark-events && \
    chown -R 185:root /opt/data /tmp/cnpj /tmp/spark-events && \
    chmod -R 775 /opt/data /tmp/cnpj /tmp/spark-events


RUN mkdir -p /opt/spark/conf && \
    echo "spark.eventLog.enabled=true" >> /opt/spark/conf/spark-defaults.conf && \
    echo "spark.eventLog.dir=/tmp/spark-events" >> /opt/spark/conf/spark-defaults.conf && \
    echo "spark.eventLog.compress=true" >> /opt/spark/conf/spark-defaults.conf


RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \; 2>/dev/null || true


USER 185

WORKDIR /opt/spark

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD pgrep -f "org.apache.spark" || exit 1


